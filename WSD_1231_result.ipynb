{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from lemminflect import getAllInflections, getAllLemmas\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import semcor\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased', top_k=10)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8293461203575134, 'purpose'),\n",
       " (0.06537206470966339, 'aim'),\n",
       " (0.026322776451706886, 'goal'),\n",
       " (0.013286333531141281, 'object'),\n",
       " (0.011476267129182816, 'function'),\n",
       " (0.009352428838610649, 'objective'),\n",
       " (0.009221878834068775, 'intention'),\n",
       " (0.0075449044816195965, 'intent'),\n",
       " (0.004589721094816923, 'task'),\n",
       " (0.004187294282019138, 'use')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test hugging face api\n",
    "sentense = \"Letters whose sole [MASK] is to make a political point will not be published.\"\n",
    "candidate = unmasker(sentense)\n",
    "result = []\n",
    "for i in range(len(candidate)):\n",
    "    result.append((candidate[i]['score'], candidate[i]['token_str']))\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAME_corpus len: 244506\n",
      "party_test_corpus len: 70\n",
      "party_train_corpus len: 637\n"
     ]
    }
   ],
   "source": [
    "# load corpus \n",
    "with open('dataset/BAWE.txt', 'r', encoding='utf-8') as f:\n",
    "    BAME_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/party_test.txt', 'r', encoding='utf-8') as f:\n",
    "    party_test_corpus = f.read().strip().split('\\n')\n",
    "with open('dataset/party_train.txt', 'r', encoding='utf-8') as f:\n",
    "    party_train_corpus = f.read().strip().split('\\n')\n",
    "    \n",
    "corpuses = [BAME_corpus, party_test_corpus, party_train_corpus]\n",
    "cor_names = [\"BAME_corpus\", \"party_test_corpus\", \"party_train_corpus\"]\n",
    "c_len = len(cor_names)\n",
    "for i in  range(c_len):\n",
    "    print(cor_names[i], \"len:\", len(corpuses[i]))\n",
    "    \n",
    "corpus_combine = BAME_corpus + party_test_corpus + party_train_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load AKL words\n",
    "with open(\"data/noun.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    noun = f.read().strip().split(', ')\n",
    "with open(\"data/adj.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    adj = f.read().strip().split(', ')\n",
    "with open(\"data/adv.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    adv = f.read().strip().split(', ')\n",
    "with open(\"data/verb.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    verb = f.read().strip().split(', ')\n",
    "with open(\"data/others.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    others = f.read().strip().split(', ')\n",
    "    \n",
    "AKL_words = [noun, adj, adv, verb, others]\n",
    "AKL_merge = noun + adj + adv + verb + others\n",
    "types = [\"noun\", \"adj\", \"adv\", \"verb\", \"others\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun words: 353\n",
      "adj words: 180\n",
      "adv words: 86\n",
      "verb words: 233\n",
      "others words: 75\n"
     ]
    }
   ],
   "source": [
    "a_len = len(AKL_words)\n",
    "for i in  range(a_len):\n",
    "    print(types[i], \"words:\", len(AKL_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    input: a string\n",
    "    output: a list\n",
    "    - transform to lower case\n",
    "    - remove the punctuation\n",
    "    - seperate the words by blank\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    punc = '!()-[]{};:\"\\,<\">./?@#$%^&*_~'\n",
    "    for p in punc: \n",
    "        text = text.replace(p, \"\")\n",
    "    return text\n",
    "\n",
    "def check_word_exist(st, base_word):\n",
    "    \"\"\"\"\n",
    "    若st 中有base_word的任何變形，回傳True\n",
    "    \"\"\"\n",
    "    tokens = st.split(' ')\n",
    "    vairation = getAllInflections(base_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "        \n",
    "    for item in var_list:\n",
    "        if item in tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def put_mask(sentense, base_word):\n",
    "    \"\"\"\n",
    "    把 [MASK] 放到第一個出現的 `base_word`各種變形\n",
    "    \"\"\"\n",
    "    tokens = sentense.split(' ')\n",
    "    vairation = getAllInflections(base_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "            \n",
    "    rep_tokens = []\n",
    "    mask = 0 # Only put mask on the first appeared base word\n",
    "    for token in tokens:\n",
    "        add = 0\n",
    "        for item in var_list:\n",
    "            if token == item and mask== 0:\n",
    "                rep_tokens.append(\"[MASK]\")\n",
    "                add = 1\n",
    "                mask += 1\n",
    "        if add == 0:\n",
    "            rep_tokens.append(token)\n",
    "\n",
    "    res_sent = \" \".join(rep_tokens)\n",
    "    return res_sent, var_list\n",
    "\n",
    "def get_candidates(sentense, base_word):\n",
    "    \"\"\"\n",
    "    所有`base_word`的變形都不會納入candidates\n",
    "    \"\"\"\n",
    "    sentense, var_list = put_mask(sentense, base_word)\n",
    "    candidate = unmasker(sentense)\n",
    "    result = {}\n",
    "    for i in range(len(candidate)):\n",
    "        same = 0\n",
    "        for item in var_list:\n",
    "            if candidate[i]['token_str'] == item:\n",
    "                same = 1\n",
    "        if same == 0:\n",
    "            result[candidate[i]['token_str']] = candidate[i]['score']\n",
    "    return result\n",
    "\n",
    "# 檢查是否是AKL字\n",
    "def check_akl(word):\n",
    "    if word in AKL_merge:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#得到相似度分數(use wup_similarity)\n",
    "def get_similarity_score(base_word, syn_word):\n",
    "    \"\"\"\n",
    "    return mean similarity score of this two words\n",
    "    compare all meaning\n",
    "    \"\"\"\n",
    "    base_sets = wn.synsets(base_word)\n",
    "    syn_sets = wn.synsets(syn_word)\n",
    "    n = len(base_sets)\n",
    "    m = len(syn_sets)\n",
    "    score = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            try:\n",
    "                score += base_sets[i].wup_similarity(syn_sets[j])\n",
    "            except:\n",
    "                pass\n",
    "    try:\n",
    "        score = score/ (n*m)\n",
    "    except:\n",
    "        score = score / 1\n",
    "    return score\n",
    "\n",
    "# 把分太細的POS 縮小分類\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "adv = ['RB', 'RBR', 'RBS']\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "# 拿到句子中的詞性\n",
    "def get_POS(sentense, target_word):\n",
    "    \"\"\"\n",
    "    回傳 `target_word` 在 `sentense`中的詞性\n",
    "    詞性種類: https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "    \"\"\"\n",
    "    # print(\"sentense: \", sentense)\n",
    "    # print(\"target_word: \", target_word)\n",
    "    tokens = nltk.word_tokenize(sentense)\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    # all variation \n",
    "    vairation = getAllInflections(target_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "    flag = 0\n",
    "    mini_pos = \"\"\n",
    "    # print(var_list)\n",
    "    for tu in tag:\n",
    "        for var in var_list:\n",
    "            if tu[0] == var:\n",
    "                mini_pos = tu[1]\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1: # found\n",
    "            break\n",
    "    # print(\"mini_pos: \", mini_pos)\n",
    "    pos = []\n",
    "    if mini_pos in verb:\n",
    "        pos.append(\"verb\")\n",
    "    if mini_pos in adj:\n",
    "        pos.append(\"adj\")\n",
    "    if mini_pos in adv:\n",
    "        pos.append(\"adv\")\n",
    "    if mini_pos in noun:\n",
    "        pos.append(\"noun\")\n",
    "#     print(pos)\n",
    "#     print('-'*10)\n",
    "    return pos \n",
    "\n",
    "def same(cand_pos ,base_pos):\n",
    "    for i in cand_pos:\n",
    "        for j in base_pos:\n",
    "            if i==j:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_all_variation(word):\n",
    "    vairation = getAllInflections(word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "    return var_list\n",
    "    \n",
    "def calculate_weight_ver2(cand, sentense, base_word):\n",
    "    \"\"\"\n",
    "    input 1: the possible words dictionary\n",
    "    input 2: the sentense used\n",
    "    input 3: base word\n",
    "    \"\"\"\n",
    "    # print(cand)\n",
    "    data_items = cand.items()\n",
    "    data_list = list(data_items)\n",
    "    cand_df = pd.DataFrame(data_list, columns=['Words', 'Score'])\n",
    "    \n",
    "    # AKL part\n",
    "    c_len = len(cand_df)\n",
    "    for i in range(c_len):\n",
    "        if check_akl(cand_df['Words'][i]):\n",
    "            cand_df['Score'][i] = cand_df['Score'][i] *1.25\n",
    "#             print(\"in AKL\")\n",
    "            \n",
    "    # POS-tagging part\n",
    "    base_pos = get_POS(sentense, base_word) #取得base的詞性\n",
    "    vairation = getAllInflections(base_word)\n",
    "    var_list = get_all_variation(base_word)\n",
    "    \n",
    "    # print(\"base_pos\", base_pos)\n",
    "    for i in range(c_len): \n",
    "        sen_tokens = sentense.split()\n",
    "        for var in var_list:\n",
    "            if var in sen_tokens:\n",
    "                sent_temp = sentense.replace(var, cand_df['Words'][i])\n",
    "                break\n",
    "        cand_pos = get_POS(sent_temp, cand_df['Words'][i]) #取得candidate的詞性\n",
    "        # print(\"cand\", cand_df['Words'][i])\n",
    "        # print(\"cand_pos\", cand_pos)\n",
    "        # print(\"sent_temp: \", sent_temp)\n",
    "        if same(cand_pos ,base_pos):\n",
    "            cand_df['Score'][i] = cand_df['Score'][i] *1.5\n",
    "            # print(\"Same type\")\n",
    "#         else:\n",
    "#             print('Not Same type')\n",
    "    # Wordnet Similarity\n",
    "    for i in range(c_len):\n",
    "        cand_df['Score'][i] += get_similarity_score(base_word, cand_df['Words'][i])\n",
    "    \n",
    "    cand_df = cand_df.sort_values(by=['Score'], ascending=False).reset_index(drop=True)\n",
    "    return cand_df\n",
    "\n",
    "# 找兩個字最近的字義\n",
    "def find_sense_of_two_words(base_word, syn_word):\n",
    "    base_word = wn.synsets(base_word) #可增加詞性 base_word = wn.synsets(base_word, pos=wn.VERB)  [VERB, NOUN, ADJ, ADV]\n",
    "    syn_word = wn.synsets(syn_word) #可增加詞性 syn_word = wn.synsets(syn_word, pos=wn.VERB)  [VERB, NOUN, ADJ, ADV]\n",
    "    wup_similarity=[]\n",
    "    wup_similarity_dict={}\n",
    "    for i in base_word:\n",
    "        for j in syn_word:\n",
    "            if wn.wup_similarity(i, j) != None:\n",
    "                wup_similarity.append(wn.wup_similarity(i, j))\n",
    "                wup_similarity_dict[wn.wup_similarity(i, j)]=[i,j]\n",
    "    \n",
    "    #找出相似度最大的值與sense\n",
    "    try:\n",
    "        similarity = max(wup_similarity)\n",
    "    except:\n",
    "        return None, None, None\n",
    "    #sense編號 \n",
    "    sense= wup_similarity_dict[max(wup_similarity)][0]\n",
    "    #字義\n",
    "    definition = wup_similarity_dict[max(wup_similarity)][0].definition()\n",
    "    \n",
    "#     sense1 = wup_similarity_dict[max(wup_similarity)][0].definition()\n",
    "#     sense2 = wup_similarity_dict[max(wup_similarity)][1].definition()\n",
    "#     print(\"sense1: \", wup_similarity_dict[max(wup_similarity)][0], sense1)\n",
    "#     print(\"sense2: \" ,  wup_similarity_dict[max(wup_similarity)][1], sense2)\n",
    "\n",
    "    return similarity, sense, definition  #propose和need相似度, propose和need相似度最接近的sense編號, 字義 \n",
    "\n",
    "\n",
    "def summary(sentence, base_word):\n",
    "    \"\"\"\n",
    "    輸入: sentence, target word\n",
    "    輸出: target word/ 例句/ 在此例句中找到最近的詞比對出來的字義 \n",
    "    \"\"\"\n",
    "    cand = get_candidates(sentence, base_word) # 找出可能的答案 \n",
    "    # print(\"candidate\", cand)\n",
    "    result_df = calculate_weight_ver2(cand, sentence, base_word) # 加權\n",
    "    r_len = len(result_df['Words'])\n",
    "    syn_final_word = \"\"\n",
    "    for i in range(r_len):\n",
    "        if(len(wn.synsets(result_df['Words'][i])))!= 0:\n",
    "            syn_final_word = result_df['Words'][i] # 拿第一名的結果\n",
    "            break\n",
    "    # print(\"base_word: \", base_word, \"syn_final_word\", syn_final_word)\n",
    "    similarity, sense, definition = find_sense_of_two_words(base_word, syn_final_word) # 找最近的字義\n",
    "    if similarity == None:\n",
    "        return None\n",
    "    \n",
    "    similar = sense.lemma_names()\n",
    "    filter = []\n",
    "    for tmp in similar:\n",
    "        if tmp!=base_word:\n",
    "            filter.append(tmp)\n",
    "    result = '、'.join(filter)\n",
    "    \n",
    "    # 印出結果\n",
    "#     print(f\"\"\"\n",
    "#     Target Word：{base_word}\n",
    "\n",
    "#     例句：{sentence}\n",
    "\n",
    "#     --------------------\n",
    "\n",
    "#     在此例句中 \"{base_word}\" 的字義是：{definition} \n",
    "    \n",
    "#     以下列出同義字：{result}\n",
    "#     \"\"\")\n",
    "    return sense\n",
    "\n",
    "    # for demo\n",
    "    # return definition, result\n",
    "\n",
    "def get_lemma(sentence_idx, word_list):\n",
    "    sent = semcor.tagged_sents(tag='sem')[sentence_idx]\n",
    "    for word in sent:\n",
    "        if(type(word)!=list):\n",
    "            # print(word)\n",
    "            for lf in word.leaves():\n",
    "                lf = lf.lower()\n",
    "                if lf in word_list:\n",
    "                    try:\n",
    "                        sense = word.label()\n",
    "                        return sense.synset()\n",
    "                    except:\n",
    "                        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the sentences\n",
    "corpus = []\n",
    "for cor in corpus_combine:\n",
    "    sentence = preprocess(cor)\n",
    "    corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_word = \"ability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of our base word sentense:  1775\n"
     ]
    }
   ],
   "source": [
    "# get the sentense that contains base_word\n",
    "filter_corpus = []\n",
    "for cor in corpus: \n",
    "    if check_word_exist(cor, base_word): \n",
    "        filter_corpus.append(cor)\n",
    "print(\"length of our base word sentense: \", len(filter_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sense: Synset('ability.n.02')\n"
     ]
    }
   ],
   "source": [
    "sentense = filter_corpus[1]\n",
    "\n",
    "answer = summary(sentense, base_word)\n",
    "print(\"sense:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://man.hubwiz.com/docset/NLTK.docset/Contents/Resources/Documents/api/nltk.corpus.reader.html#module-nltk.corpus.reader.semcor\n",
    "\n",
    "https://www.nltk.org/api/nltk.corpus.reader.semcor.html\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/corpus/reader/semcor.html\n",
    "\n",
    "https://www.nltk.org/howto/corpus.html#chunked-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37176"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sents = len(semcor.sents()) # total number of sentences\n",
    "num_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(semcor.sents()[0]) # sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/sencor_sentenses.txt', 'r', encoding='utf-8') as f:\n",
    "    semcor_text = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the sentences\n",
    "semcor_corpus = []\n",
    "for cor in semcor_text:\n",
    "    sentence = preprocess(cor)\n",
    "    semcor_corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_words = [\"star\", \"galley\", \"cone\", \"bass\", \"bow\", \"taste\", \"interest\", \"issue\", \"duty\", \"sentence\", \"slug\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of base word \"star\" sentense: 49\n",
      "length of base word \"galley\" sentense: 2\n",
      "length of base word \"cone\" sentense: 15\n",
      "length of base word \"bass\" sentense: 13\n",
      "length of base word \"bow\" sentense: 22\n",
      "length of base word \"taste\" sentense: 52\n",
      "length of base word \"interest\" sentense: 382\n",
      "length of base word \"issue\" sentense: 168\n",
      "length of base word \"duty\" sentense: 71\n",
      "length of base word \"sentence\" sentense: 49\n",
      "length of base word \"slug\" sentense: 16\n"
     ]
    }
   ],
   "source": [
    "s_len = len(semcor_corpus)\n",
    "for word in candidates_words:\n",
    "    base_word = word\n",
    "    filter_cor_idx = []\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in range(s_len): \n",
    "        if check_word_exist(semcor_corpus[i], base_word): \n",
    "            filter_cor_idx.append(i)\n",
    "    print(f\"length of base word \\\"{base_word}\\\" sentense: {len(filter_cor_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of base word \"star\" sentense: 49\n",
      "star's precision: 0.2222222222222222\n",
      "----------\n",
      "length of base word \"galley\" sentense: 2\n",
      "Not enough data.\n",
      "----------\n",
      "length of base word \"cone\" sentense: 15\n",
      "cone's precision: 0.3333333333333333\n",
      "----------\n",
      "length of base word \"bass\" sentense: 13\n",
      "bass's precision: 0.0\n",
      "----------\n",
      "length of base word \"bow\" sentense: 22\n",
      "bow's precision: 0.3684210526315789\n",
      "----------\n",
      "length of base word \"taste\" sentense: 52\n",
      "taste's precision: 0.41935483870967744\n",
      "----------\n",
      "length of base word \"interest\" sentense: 382\n",
      "interest's precision: 0.1787709497206704\n",
      "----------\n",
      "length of base word \"issue\" sentense: 168\n",
      "issue's precision: 0.17105263157894737\n",
      "----------\n",
      "length of base word \"duty\" sentense: 71\n",
      "duty's precision: 0.4782608695652174\n",
      "----------\n",
      "length of base word \"sentence\" sentense: 49\n",
      "sentence's precision: 0.4666666666666667\n",
      "----------\n",
      "length of base word \"slug\" sentense: 16\n",
      "slug's precision: 0.6\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "s_len = len(semcor_corpus)\n",
    "for word in candidates_words:\n",
    "    base_word = word\n",
    "    filter_cor_idx = []\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in range(s_len): \n",
    "        if check_word_exist(semcor_corpus[i], base_word): \n",
    "            filter_cor_idx.append(i)\n",
    "    print(f\"length of base word \\\"{base_word}\\\" sentense: {len(filter_cor_idx)}\")\n",
    "    for idx in filter_cor_idx:\n",
    "        sentense = semcor_corpus[idx]\n",
    "        answer = summary(sentense, base_word)\n",
    "        var_list = get_all_variation(base_word)\n",
    "        ground_true = get_lemma(idx, var_list)\n",
    "        if ground_true is None or answer is None:\n",
    "            continue\n",
    "        # print(answer, ground_true, idx)\n",
    "        if answer == ground_true:\n",
    "            # print(\"Same!\")\n",
    "            numerator += 1\n",
    "            denominator += 1\n",
    "        else:\n",
    "            denominator += 1\n",
    "    if denominator!= 0:\n",
    "        print(f\"{base_word}'s precision: {numerator/denominator}\")\n",
    "    else:\n",
    "        print(\"Not enough data.\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slug'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35320"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`` running around in the moonlight almost naked and slugging a man with a rock '' \""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_corpus[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`` Running around in the moonlight almost naked and slugging a man with a rock '' ?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(semcor.sents()[idx]) # sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('slug.v.01')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = summary(sentense, base_word)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_true = get_lemma(idx, var_list)\n",
    "if ground_true is None:\n",
    "    print(\"not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['``'],\n",
       " Tree(Lemma('frolic.v.01.run_around'), ['Running', 'around']),\n",
       " ['in'],\n",
       " ['the'],\n",
       " ['moonlight'],\n",
       " ['almost'],\n",
       " ['naked'],\n",
       " ['and'],\n",
       " Tree(Lemma('slug.v.01.slug'), ['slugging']),\n",
       " ['a'],\n",
       " ['man'],\n",
       " ['with'],\n",
       " ['a'],\n",
       " ['rock'],\n",
       " [\"''\"],\n",
       " ['?']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bug in semcor -> bowed.s.00 不是lemma\n",
    "semcor.tagged_sents(tag='sem')[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
