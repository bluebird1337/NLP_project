{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from lemminflect import getAllInflections, getAllLemmas\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased', top_k=10)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8293458223342896, 'purpose'),\n",
       " (0.06537234783172607, 'aim'),\n",
       " (0.02632279321551323, 'goal'),\n",
       " (0.013286354951560497, 'object'),\n",
       " (0.011476273648440838, 'function'),\n",
       " (0.00935242511332035, 'objective'),\n",
       " (0.00922190211713314, 'intention'),\n",
       " (0.007544944994151592, 'intent'),\n",
       " (0.004589710384607315, 'task'),\n",
       " (0.004187312442809343, 'use')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sentense = \"Letters whose sole [MASK] is to make a political point will not be published.\"\n",
    "candidate = unmasker(sentense)\n",
    "result = []\n",
    "for i in range(len(candidate)):\n",
    "    result.append((candidate[i]['score'], candidate[i]['token_str']))\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAME_corpus len: 244506\n",
      "big_corpus len: 31564\n",
      "paper_corpus len: 123656\n",
      "party_test_corpus len: 70\n",
      "party_train_corpus len: 637\n"
     ]
    }
   ],
   "source": [
    "# load corpus \n",
    "with open('dataset/BAWE.txt', 'r', encoding='utf-8') as f:\n",
    "    BAME_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/big.txt', 'r', encoding='utf-8') as f:\n",
    "    big_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/paper.txt', 'r', encoding='utf-8') as f:\n",
    "    paper_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/party_test.txt', 'r', encoding='utf-8') as f:\n",
    "    party_test_corpus = f.read().strip().split('\\n')\n",
    "with open('dataset/party_train.txt', 'r', encoding='utf-8') as f:\n",
    "    party_train_corpus = f.read().strip().split('\\n')\n",
    "    \n",
    "corpuses = [BAME_corpus, big_corpus, paper_corpus, party_test_corpus, party_train_corpus]\n",
    "cor_names = [\"BAME_corpus\", \"big_corpus\", \"paper_corpus\", \"party_test_corpus\", \"party_train_corpus\"]\n",
    "c_len = len(cor_names)\n",
    "for i in  range(c_len):\n",
    "    print(cor_names[i], \"len:\", len(corpuses[i]))\n",
    "    \n",
    "corpus_combine = BAME_corpus + big_corpus + paper_corpus + party_test_corpus + party_train_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load AKL words\n",
    "with open(\"data/noun.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    noun = f.read().strip().split(', ')\n",
    "with open(\"data/adj.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    adj = f.read().strip().split(', ')\n",
    "with open(\"data/adv.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    adv = f.read().strip().split(', ')\n",
    "with open(\"data/verb.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    verb = f.read().strip().split(', ')\n",
    "with open(\"data/others.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    others = f.read().strip().split(', ')\n",
    "    \n",
    "AKL_words = [noun, adj, adv, verb, others]\n",
    "AKL_merge = noun + adj + adv + verb + others\n",
    "types = [\"noun\", \"adj\", \"adv\", \"verb\", \"others\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun words: 353\n",
      "adj words: 180\n",
      "adv words: 86\n",
      "verb words: 233\n",
      "others words: 75\n"
     ]
    }
   ],
   "source": [
    "a_len = len(AKL_words)\n",
    "for i in  range(a_len):\n",
    "    print(types[i], \"words:\", len(AKL_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the sentences\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    input: a string\n",
    "    output: a list\n",
    "    - transform to lower case\n",
    "    - remove the punctuation\n",
    "    - seperate the words by blank\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    punc = '!()-[]{};:\"\\,<\">./?@#$%^&*_~1234567890'\n",
    "    for p in punc: \n",
    "        text = text.replace(p, \"\")\n",
    "    return text\n",
    "\n",
    "corpus = []\n",
    "for cor in corpus_combine:\n",
    "    sentence = preprocess(cor)\n",
    "    corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: get possible candidate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_word = \"ability\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NNS': ('abilities', 'ability'), 'NN': ('ability',)}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAllInflections(base_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_exist(st, base_word):\n",
    "    \"\"\"\"\n",
    "    若st 中有base_word的任何變形，回傳True\n",
    "    \"\"\"\n",
    "    tokens = st.split(' ')\n",
    "    vairation = getAllInflections(base_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "        \n",
    "    for item in var_list:\n",
    "        if item in tokens:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_mask(sentense, base_word):\n",
    "    \"\"\"\n",
    "    把 [MASK] 放到第一個出現的 `base_word`各種變形\n",
    "    \"\"\"\n",
    "    tokens = sentense.split(' ')\n",
    "    vairation = getAllInflections(base_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "            \n",
    "    rep_tokens = []\n",
    "    mask = 0 # Only put mask on the first appeared base word\n",
    "    for token in tokens:\n",
    "        add = 0\n",
    "        for item in var_list:\n",
    "            if token == item and mask== 0:\n",
    "                rep_tokens.append(\"[MASK]\")\n",
    "                add = 1\n",
    "                mask += 1\n",
    "        if add == 0:\n",
    "            rep_tokens.append(token)\n",
    "\n",
    "    res_sent = \" \".join(rep_tokens)\n",
    "    return res_sent, var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(sentense, base_word):\n",
    "    \"\"\"\n",
    "    所有`base_word`的變形都不會納入candidates\n",
    "    \"\"\"\n",
    "    sentense, var_list = put_mask(sentense, base_word)\n",
    "    candidate = unmasker(sentense)\n",
    "    result = {}\n",
    "    for i in range(len(candidate)):\n",
    "        same = 0\n",
    "        for item in var_list:\n",
    "            if candidate[i]['token_str'] == item:\n",
    "                same = 1\n",
    "        if same == 0:\n",
    "            result[candidate[i]['token_str']] = candidate[i]['score']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of our base word sentense:  2345\n"
     ]
    }
   ],
   "source": [
    "# get the sentense that contains base_word\n",
    "filter_corpus = []\n",
    "for cor in corpus: \n",
    "    if check_word_exist(cor, base_word): \n",
    "        filter_corpus.append(cor)\n",
    "print(\"length of our base word sentense: \", len(filter_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capacity': 0.023000137880444527,\n",
       " 'freedom': 0.006012896075844765,\n",
       " 'willingness': 0.004808926954865456,\n",
       " 'capability': 0.004298985470086336,\n",
       " 'desire': 0.0022593154571950436,\n",
       " 'opportunity': 0.001206710352562368,\n",
       " 'skills': 0.0008579287678003311,\n",
       " 'determination': 0.0008293994469568133}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_sentense = filter_corpus[0]\n",
    "cand = get_candidates(used_sentense, base_word)\n",
    "cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Processing weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_akl(word):\n",
    "    if word in AKL_merge:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS(sentense, target_word):\n",
    "    \"\"\"\n",
    "    回傳 `target_word` 在 `sentense`中的詞性\n",
    "    詞性種類: https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(sentense)\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    for tu in tag:\n",
    "        if tu[0] == target_word:\n",
    "            return tu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(base_word, syn_word):\n",
    "    \"\"\"\n",
    "    return mean similarity score of this two words\n",
    "    compare all meaning\n",
    "    \"\"\"\n",
    "    base_sets = wn.synsets(base_word)\n",
    "    syn_sets = wn.synsets(syn_word)\n",
    "    n = len(base_sets)\n",
    "    m = len(syn_sets)\n",
    "    score = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            try:\n",
    "                score += base_sets[i].wup_similarity(syn_sets[j])\n",
    "            except:\n",
    "                pass\n",
    "    score = score/ (n*m)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO 3 ]**</font> wordnet 有好幾種算相似度的方法，哪個最適合?\n",
    "\n",
    "- path_similarity\n",
    "\n",
    "-  lch_similarity\n",
    "\n",
    "-  wup_similarity\n",
    "\n",
    "https://www.nltk.org/howto/wordnet.html#similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- path_similarity:\n",
    "    \n",
    "    Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hyponym) taxonomy.\n",
    "    \n",
    "    -> 檢查是否有上下位關係 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lch_similarity: \n",
    "\n",
    "    based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. The relationship is given as -log(p/2d) where p is the shortest path length and d the taxonomy depth.\n",
    "    \n",
    "    -> 與上類似，算法不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wup_similarity: \n",
    "    based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node). \n",
    "    \n",
    "    -> 回傳最接近的ancestor深度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight(cand, sentense, base_word):\n",
    "    \"\"\"\n",
    "    input 1: the possible words dictionary\n",
    "    input 2: the sentense used\n",
    "    input 3: base word\n",
    "    \"\"\"\n",
    "    data_items = cand.items()\n",
    "    data_list = list(data_items)\n",
    "    cand_df = pd.DataFrame(data_list, columns=['Words', 'Score'])\n",
    "    \n",
    "    # AKL part\n",
    "    c_len = len(cand_df)\n",
    "    for i in range(c_len):\n",
    "        if check_akl(cand_df['Words'][i]):\n",
    "            cand_df['Score'][i] = cand_df['Score'][i] *1.25\n",
    "            print(\"in AKL\")\n",
    "            \n",
    "    # POS-tagging part\n",
    "    base_pos = get_POS(sentense, base_word)\n",
    "    for i in range(c_len): \n",
    "        cand_pos = get_POS(sentense, cand_df['Words'][i])\n",
    "        if cand_pos == base_pos:\n",
    "            cand_df['Score'][i] = cand_df['Score'][i] *1.5\n",
    "            print(\"Same type\")\n",
    "    \n",
    "    # Wordnet Similarity\n",
    "    for i in range(c_len):\n",
    "        cand_df['Score'][i] += get_similarity_score(base_word, cand_df['Words'][i])\n",
    "    \n",
    "    cand_df = cand_df.sort_values(by=['Score'], ascending=False).reset_index(drop=True)\n",
    "    return cand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO 4 ]**</font> nltk 詞性分得太細了，怎麼降低標準 (詞性加權無法使用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in AKL\n",
      "in AKL\n",
      "in AKL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>skills</td>\n",
       "      <td>0.637222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capability</td>\n",
       "      <td>0.549754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>freedom</td>\n",
       "      <td>0.464346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>capacity</td>\n",
       "      <td>0.431046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>determination</td>\n",
       "      <td>0.430224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>willingness</td>\n",
       "      <td>0.421476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>opportunity</td>\n",
       "      <td>0.418175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>desire</td>\n",
       "      <td>0.211564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Words     Score\n",
       "0         skills  0.637222\n",
       "1     capability  0.549754\n",
       "2        freedom  0.464346\n",
       "3       capacity  0.431046\n",
       "4  determination  0.430224\n",
       "5    willingness  0.421476\n",
       "6    opportunity  0.418175\n",
       "7         desire  0.211564"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input : cand, used_sentense, base_word\n",
    "result_df = calculate_weight(cand, used_sentense, base_word)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_final_word = result_df['Words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Find the closest meaning between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sense_of_two_words(base_word, syn_word):\n",
    "    base_word = wn.synsets(base_word) #可增加詞性 base_word = wn.synsets(base_word, pos=wn.VERB)  [VERB, NOUN, ADJ, ADV]\n",
    "    syn_word = wn.synsets(syn_word) #可增加詞性 syn_word = wn.synsets(syn_word, pos=wn.VERB)  [VERB, NOUN, ADJ, ADV]\n",
    "    \n",
    "    path_similarity=[]\n",
    "    path_similarity_dict={}\n",
    "    for i in base_word:\n",
    "        for j in syn_word:\n",
    "            path_similarity.append(wn.path_similarity(i, j))\n",
    "            path_similarity_dict[wn.path_similarity(i, j)]=[i,j]\n",
    "            \n",
    "    #找出相似度最大的值與sense    \n",
    "    similarity = max(path_similarity)\n",
    "    #propose sense編號 \n",
    "    sense= path_similarity_dict[max(path_similarity)][0]\n",
    "    #propose 字義\n",
    "    definition = path_similarity_dict[max(path_similarity)][0].definition()\n",
    "  \n",
    "    return similarity, sense, definition  #propose和need相似度, propose和need相似度最接近的sense編號, 字義 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity, sense, definition = find_sense_of_two_words(base_word, syn_final_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target Word：ability\n",
      "\n",
      "例句：the teacher encouraged them to discuss their ideas and again  advocates this  the quality of pupils' mathematical thinking as well as their ability to express themselves are considerably enhanced by discussion\n",
      "\n",
      "--------------------\n",
      "\n",
      "在此例句中 \"ability\" 字義：possession of the qualities (especially mental qualities) required to do something or get something done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Target Word：{base_word}\n",
    "\n",
    "例句：{used_sentense}\n",
    "\n",
    "--------------------\n",
    "\n",
    "在此例句中 \"{base_word}\" 字義：{definition}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://man.hubwiz.com/docset/NLTK.docset/Contents/Resources/Documents/api/nltk.corpus.reader.html#module-nltk.corpus.reader.semcor\n",
    "\n",
    "https://www.nltk.org/api/nltk.corpus.reader.semcor.html\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/corpus/reader/semcor.html\n",
    "\n",
    "https://www.nltk.org/howto/corpus.html#chunked-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\WangHongWen\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('semcor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semcor.sents()) # total number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(semcor.sents()[0]) # sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('PRP', ['It']),\n",
       " Tree('VB', ['recommended']),\n",
       " Tree('IN', ['that']),\n",
       " Tree('NNP', ['Fulton']),\n",
       " Tree('NN', ['legislators']),\n",
       " Tree('VB', ['act']),\n",
       " Tree(None, ['``']),\n",
       " Tree('TO', ['to']),\n",
       " Tree('VB', ['have']),\n",
       " Tree('DT', ['these']),\n",
       " Tree('NN', ['laws']),\n",
       " Tree('VB', ['studied']),\n",
       " Tree('CC', ['and']),\n",
       " Tree('VB', ['revised']),\n",
       " Tree('TO', ['to']),\n",
       " Tree('DT', ['the']),\n",
       " Tree('NN', ['end']),\n",
       " Tree('IN', ['of']),\n",
       " Tree('VB', ['modernizing']),\n",
       " Tree('CC', ['and']),\n",
       " Tree('VB', ['improving']),\n",
       " Tree('PRP', ['them']),\n",
       " Tree(None, [\"''\"]),\n",
       " Tree(None, ['.'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor.tagged_sents(tag='pos')[5] # POS-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The'],\n",
       " Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]),\n",
       " Tree(Lemma('state.v.01.say'), ['said']),\n",
       " Tree(Lemma('friday.n.01.Friday'), ['Friday']),\n",
       " ['an'],\n",
       " Tree(Lemma('probe.n.01.investigation'), ['investigation']),\n",
       " ['of'],\n",
       " Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']),\n",
       " [\"'s\"],\n",
       " Tree(Lemma('late.s.03.recent'), ['recent']),\n",
       " Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']),\n",
       " Tree(Lemma('produce.v.04.produce'), ['produced']),\n",
       " ['``'],\n",
       " ['no'],\n",
       " Tree(Lemma('evidence.n.01.evidence'), ['evidence']),\n",
       " [\"''\"],\n",
       " ['that'],\n",
       " ['any'],\n",
       " Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']),\n",
       " Tree(Lemma('happen.v.01.take_place'), ['took', 'place']),\n",
       " ['.']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor.tagged_sents(tag='sem')[0] # wordnet sense-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
