{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased', top_k=10)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8293458223342896, 'purpose'),\n",
       " (0.06537234783172607, 'aim'),\n",
       " (0.02632279321551323, 'goal'),\n",
       " (0.013286354951560497, 'object'),\n",
       " (0.011476273648440838, 'function'),\n",
       " (0.00935242511332035, 'objective'),\n",
       " (0.00922190211713314, 'intention'),\n",
       " (0.007544944994151592, 'intent'),\n",
       " (0.004589710384607315, 'task'),\n",
       " (0.004187312442809343, 'use')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sentense = \"Letters whose sole [MASK] is to make a political point will not be published.\"\n",
    "candidate = unmasker(sentense)\n",
    "result = []\n",
    "for i in range(len(candidate)):\n",
    "    result.append((candidate[i]['score'], candidate[i]['token_str']))\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAME_corpus len: 244506\n",
      "big_corpus len: 31564\n",
      "paper_corpus len: 123656\n",
      "party_test_corpus len: 70\n",
      "party_train_corpus len: 637\n"
     ]
    }
   ],
   "source": [
    "# load corpus \n",
    "with open('dataset/BAWE.txt', 'r', encoding='utf-8') as f:\n",
    "    BAME_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/big.txt', 'r', encoding='utf-8') as f:\n",
    "    big_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/paper.txt', 'r', encoding='utf-8') as f:\n",
    "    paper_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/party_test.txt', 'r', encoding='utf-8') as f:\n",
    "    party_test_corpus = f.read().strip().split('\\n')\n",
    "with open('dataset/party_train.txt', 'r', encoding='utf-8') as f:\n",
    "    party_train_corpus = f.read().strip().split('\\n')\n",
    "    \n",
    "corpuses = [BAME_corpus, big_corpus, paper_corpus, party_test_corpus, party_train_corpus]\n",
    "cor_names = [\"BAME_corpus\", \"big_corpus\", \"paper_corpus\", \"party_test_corpus\", \"party_train_corpus\"]\n",
    "c_len = len(cor_names)\n",
    "for i in  range(c_len):\n",
    "    print(cor_names[i], \"len:\", len(corpuses[i]))\n",
    "    \n",
    "corpus_combine = BAME_corpus + big_corpus + paper_corpus + party_test_corpus + party_train_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load AKL words\n",
    "with open(\"data/noun.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    noun = f.read().strip().split(', ')\n",
    "with open(\"data/adj.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    adj = f.read().strip().split(', ')\n",
    "with open(\"data/adv.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    adv = f.read().strip().split(', ')\n",
    "with open(\"data/verb.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    verb = f.read().strip().split(', ')\n",
    "with open(\"data/others.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    others = f.read().strip().split(', ')\n",
    "    \n",
    "AKL_words = [noun, adj, adv, verb, others]\n",
    "AKL_merge = noun + adj + adv + verb + others\n",
    "types = [\"noun\", \"adj\", \"adv\", \"verb\", \"others\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun words: 353\n",
      "adj words: 180\n",
      "adv words: 86\n",
      "verb words: 233\n",
      "others words: 75\n"
     ]
    }
   ],
   "source": [
    "a_len = len(AKL_words)\n",
    "for i in  range(a_len):\n",
    "    print(types[i], \"words:\", len(AKL_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the sentences\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    input: a string\n",
    "    output: a list\n",
    "    - transform to lower case\n",
    "    - remove the punctuation\n",
    "    - seperate the words by blank\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    punc = '!()-[]{};:\"\\,<\">./?@#$%^&*_~1234567890'\n",
    "    for p in punc: \n",
    "        text = text.replace(p, \"\")\n",
    "    return text\n",
    "\n",
    "corpus = []\n",
    "for cor in corpus_combine:\n",
    "    sentence = preprocess(cor)\n",
    "    corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: get possible candidate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_word = \"ability\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO 1 ]**</font> 如何納入動詞、名詞的變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_single_word(st, base_word):\n",
    "    tokens = st.split(' ')\n",
    "    if base_word in tokens:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_mask(sentense, base_word):\n",
    "    \"\"\"\n",
    "    put mask on the target word\n",
    "    \"\"\"\n",
    "    tokens = sentense.split(' ')\n",
    "    rep_tokens = [\"[MASK]\" if word==base_word else word for word in tokens]\n",
    "    \n",
    "    res_sent = \" \".join(rep_tokens)\n",
    "    return res_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(sentense, base_word):\n",
    "    sentense = put_mask(sentense, base_word)\n",
    "    candidate = unmasker(sentense)\n",
    "    result = {}\n",
    "    for i in range(len(candidate)):\n",
    "        if candidate[i]['token_str'] == base_word:\n",
    "            continue\n",
    "        result[candidate[i]['token_str']] = candidate[i]['score']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of our base word sentense:  2071\n"
     ]
    }
   ],
   "source": [
    "# get the sentense that contains base_word\n",
    "filter_corpus = []\n",
    "for cor in corpus: \n",
    "    if check_single_word(cor, base_word): \n",
    "        filter_corpus.append(cor)\n",
    "print(\"length of our base word sentense: \", len(filter_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO 2 ]**</font> 套入其他例句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capacity': 0.023000137880444527,\n",
       " 'abilities': 0.007775991223752499,\n",
       " 'freedom': 0.006012896075844765,\n",
       " 'willingness': 0.004808926954865456,\n",
       " 'capability': 0.004298985470086336,\n",
       " 'desire': 0.0022593154571950436,\n",
       " 'opportunity': 0.001206710352562368,\n",
       " 'skills': 0.0008579287678003311,\n",
       " 'determination': 0.0008293994469568133}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_sentense = filter_corpus[0]\n",
    "cand = get_candidates(used_sentense, base_word)\n",
    "cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Processing weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_akl(word):\n",
    "    if word in AKL_merge:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS(sentense, target_word):\n",
    "    \"\"\"\n",
    "    回傳 `target_word` 在 `sentense`中的詞性\n",
    "    詞性種類: https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(sentense)\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    for tu in tag:\n",
    "        if tu[0] == target_word:\n",
    "            return tu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(base_word, syn_word):\n",
    "    \"\"\"\n",
    "    return mean similarity score of this two words\n",
    "    compare all meaning\n",
    "    \"\"\"\n",
    "    base_sets = wn.synsets(base_word)\n",
    "    syn_sets = wn.synsets(syn_word)\n",
    "    n = len(base_sets)\n",
    "    m = len(syn_sets)\n",
    "    score = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            try:\n",
    "                score += base_sets[i].path_similarity(syn_sets[j])\n",
    "            except:\n",
    "                pass\n",
    "    score = score/ (n*m)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO 3 ]**</font> wordnet 有好幾種算相似度的方法，哪個最適合?\n",
    "\n",
    "- path_similarity\n",
    "\n",
    "-  lch_similarity\n",
    "\n",
    "-  wup_similarity\n",
    "\n",
    "https://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight(cand, sentense, base_word):\n",
    "    \"\"\"\n",
    "    input 1: the possible words dictionary\n",
    "    input 2: the sentense used\n",
    "    input 3: base word\n",
    "    \"\"\"\n",
    "    data_items = cand.items()\n",
    "    data_list = list(data_items)\n",
    "    cand_df = pd.DataFrame(data_list, columns=['Words', 'Score'])\n",
    "    \n",
    "    # AKL part\n",
    "    c_len = len(cand_df)\n",
    "    for i in range(c_len):\n",
    "        if check_akl(cand_df['Words'][i]):\n",
    "            cand_df['Score'][i] = cand_df['Score'][i] *1.25\n",
    "            print(\"in AKL\")\n",
    "            \n",
    "    # POS-tagging part\n",
    "    base_pos = get_POS(sentense, base_word)\n",
    "    for i in range(c_len): \n",
    "        cand_pos = get_POS(sentense, cand_df['Words'][i])\n",
    "        if cand_pos == base_pos:\n",
    "            cand_df['Score'][i] = cand_df['Score'][i] *1.5\n",
    "            print(\"Same type\")\n",
    "    \n",
    "    # Wordnet Similarity\n",
    "    for i in range(c_len):\n",
    "        cand_df['Score'][i] += get_similarity_score(base_word, cand_df['Words'][i])\n",
    "    \n",
    "    cand_df = cand_df.sort_values(by=['Score'], ascending=False).reset_index(drop=True)\n",
    "    return cand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO 4 ]**</font> nltk 詞性分得太細了，怎麼降低標準 (詞性加權無法使用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in AKL\n",
      "in AKL\n",
      "in AKL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abilities</td>\n",
       "      <td>0.579205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skills</td>\n",
       "      <td>0.313358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capability</td>\n",
       "      <td>0.224868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>capacity</td>\n",
       "      <td>0.175288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>freedom</td>\n",
       "      <td>0.155219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>willingness</td>\n",
       "      <td>0.131793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>opportunity</td>\n",
       "      <td>0.128493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>determination</td>\n",
       "      <td>0.128144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>desire</td>\n",
       "      <td>0.066479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Words     Score\n",
       "0      abilities  0.579205\n",
       "1         skills  0.313358\n",
       "2     capability  0.224868\n",
       "3       capacity  0.175288\n",
       "4        freedom  0.155219\n",
       "5    willingness  0.131793\n",
       "6    opportunity  0.128493\n",
       "7  determination  0.128144\n",
       "8         desire  0.066479"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input : cand, used_sentense, base_word\n",
    "result_df = calculate_weight(cand, used_sentense, base_word)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_final_word = result_df['Words'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Find the closest meaning between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sense_of_two_words(base_word, syn_word):\n",
    "    base_word = wn.synsets(base_word) #可增加詞性 base_word = wn.synsets(base_word, pos=wn.VERB)  [VERB, NOUN, ADJ, ADV]\n",
    "    syn_word = wn.synsets(syn_word) #可增加詞性 syn_word = wn.synsets(syn_word, pos=wn.VERB)  [VERB, NOUN, ADJ, ADV]\n",
    "    \n",
    "    path_similarity=[]\n",
    "    path_similarity_dict={}\n",
    "    for i in base_word:\n",
    "        for j in syn_word:\n",
    "            path_similarity.append(wn.path_similarity(i, j))\n",
    "            path_similarity_dict[wn.path_similarity(i, j)]=[i,j]\n",
    "            \n",
    "    #找出相似度最大的值與sense    \n",
    "    similarity = max(path_similarity)\n",
    "    #propose sense編號 \n",
    "    sense= path_similarity_dict[max(path_similarity)][0]\n",
    "    #propose 字義\n",
    "    definition = path_similarity_dict[max(path_similarity)][0].definition()\n",
    "  \n",
    "    return similarity, sense, definition  #propose和need相似度, propose和need相似度最接近的sense編號, 字義 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity, sense, definition = find_sense_of_two_words(base_word, syn_final_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target Word：ability\n",
      "\n",
      "例句：the teacher encouraged them to discuss their ideas and again  advocates this  the quality of pupils' mathematical thinking as well as their ability to express themselves are considerably enhanced by discussion\n",
      "\n",
      "--------------------\n",
      "\n",
      "在此例句中 \"ability\" 字義：possession of the qualities (especially mental qualities) required to do something or get something done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Target Word：{base_word}\n",
    "\n",
    "例句：{used_sentense}\n",
    "\n",
    "--------------------\n",
    "\n",
    "在此例句中 \"{base_word}\" 字義：{definition}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
